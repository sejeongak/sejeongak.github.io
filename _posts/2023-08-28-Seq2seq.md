---
title: Sequence to Sequence Learning with Neural Networks
date: 2023-08-28 14:49:00 +09:00
categories: [AI, 논문]
tags:
  [ Seq2Seq ]
---

# Sequence to Sequence Learning with Neural Networks

## Abstract

- Deep Neural Networks는 어려운 task에 훌륭한 성능을 달성하는 효과적인 모델입니다.
- DNN(Deep Neural Networks)은 많은 양의 레이블된 학습 데이터 세트가 있는 경우에 효과적이지만, sequence를 sequence로 매핑에는 한계가 있습니다.
  - 즉 입력 sequence와 출력 sequence가 모두 시간적인 관계를 갖는 경우에 사용할 수 없습니다.
    
- 이 논문에서는 sequence 구조를 학습하 일반적인 end-to-end 방법론을 제시합니다.
- Mylti-layered LSTM을 사용하여 input sequence를 고정된차원의 벡터로 매핑하고, 해당 벡터에서 target sequence로 디코딩하기 위해 또 다른 LSTM을 사용합니다.
  - LSTM은 장기 의존성을 처리할 수 있기 때문에 sequence에서 패턴을 인식하고 예측할 수 있습니다.
  - LSTM은 단어의 순서를 고려하여 문장의 의미를 파악하며, 능동태와 수동태의 차이와 같은 문법적인 변화에 대해서도 인식할 수 있습니다.
    
- 이 논문에서는 WMT'14 데이터셋을 사용한 영어-프랑스어 번역 작업에서 34.8의 BLEU 점수를 달성합니다.
  - BLEU 점수: machine translation의 성능을 측정하는 데 사용되는 지표 중 하나이며, 값이 높을수록 번역의 질이 더 높다는 것을 의미합니다.

## 1. Introduction

- Deep Neural Networks는 speech recognition, visual object recognition 등 어려운 task에 대해서 훌륭한 성능을 내는 강력한 머신러닝 모델입니다.
- DNN은 임의의 병렬 계산을 적은 수의 단계로 수행할 수 있기 때문에 강력합니다. 그러나 DNN은 input 데이터가 가변적인 길이를 가지는 경우에는 한계점이 존재하기 때문에 input과 output이 고정된 차원의 벡터로 인코딩하는 문제에 대해서만 적용이 가능합니다.
- 예를 들어, speech recognition과 machine translation은 sequential한 문제입니다. 마찬가지로, question answering 역시 질문 sequence를 답변 sequence로 매핑하는 문제입니다. 이러한 sequential한 특성을 고려할 때 DNN의 한계가 드러납니다.
- 이 논문에서는 시퀀스를 처리하는 데 매우 유용한 네트워크 아키텍쳐인 LSTM을 사용해 DNN의 한계를 보완했습니다. LSTM은 장기간의 시간적 의존성이 있는 데이터에서 성공적으로 학습할 수 있는 능력을 가지고 있으므로, input과 이에 해당하는 output 간의 상당한 시간 지연이 있는 경우에도 유용합니다. 
- LSTM을 사용하여 input sequence를 고정된 차원의 벡터로 인코딩하고, 그 후에 디코딩 단계에서 해당 벡터를 output sequence로 디코딩하여 sequence를 sequence로 매핑합니다. 이를 통해 input sequence와 output sequence의 길이가 가변적인 문제를 해결할 수 있습니다.
- LSTM을 사용해서 input sequence를 한 스텝씩 읽어 큰 고정 차원 벡터 표현을 얻고, 그 다음에 이 벡터에서 output sequence를 추출하기 위해 다른 LSTM을 사용합니다. 
![Fig1](/img/fig1.png)
