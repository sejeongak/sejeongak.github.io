---
title: Sequence to Sequence Learning with Neural Networks
date: 2023-08-28 14:49:00 +09:00
categories: [AI, 논문]
tags:
  [ Seq2Seq ]
---

# Sequence to Sequence Learning with Neural Networks

## Abstract

- Deep Neural Networks는 어려운 task에 훌륭한 성능을 달성하는 효과적인 모델입니다.
- DNN(Deep Neural Networks)은 많은 양의 레이블된 학습 데이터 세트가 있는 경우에 효과적이지만, sequence를 sequence로 매핑에는 한계가 있습니다.
  - 즉 입력 sequence와 출력 sequence가 모두 시간적인 관계를 갖는 경우에 사용할 수 없습니다.
    
- 이 논문에서는 sequence 구조를 학습하는 일반적인 end-to-end 방법론을 제시합니다.
- Mylti-layered LSTM을 사용하여 input sequence를 고정된 차원의 벡터로 매핑하고, 해당 벡터에서 target sequence로 디코딩하기 위해 또 다른 LSTM을 사용합니다.
  - LSTM은 장기 의존성을 처리할 수 있기 때문에 sequence에서 패턴을 인식하고 예측할 수 있습니다.
  - LSTM은 단어의 순서를 고려하여 문장의 의미를 파악하며, 능동태와 수동태의 차이와 같은 문법적인 변화에 대해서도 인식할 수 있습니다.
    
- 이 논문에서는 WMT'14 데이터셋을 사용한 영어-프랑스어 번역 작업에서 34.8의 BLEU 점수를 달성합니다.
  - BLEU 점수: machine translation의 성능을 측정하는 데 사용되는 지표 중 하나이며, 값이 높을수록 번역의 질이 더 높다는 것을 의미합니다.

## 1. Introduction

- Deep Neural Networks는 speech recognition, visual object recognition 등 어려운 task에 대해서 훌륭한 성능을 내는 강력한 머신러닝 모델입니다.
- DNN은 임의의 병렬 계산을 적은 수의 단계로 수행할 수 있기 때문에 강력합니다. 그러나 DNN은 input 데이터가 가변적인 길이를 가지는 경우에는 한계점이 존재하기 때문에 input과 output이 고정된 차원의 벡터로 인코딩하는 문제에 대해서만 적용이 가능합니다.
- 예를 들어, speech recognition과 machine translation은 sequential한 문제입니다. 마찬가지로, question answering 역시 질문 sequence를 답변 sequence로 매핑하는 문제입니다. 이러한 sequential한 특성을 고려할 때 DNN의 한계가 드러납니다.
- 이 논문에서는 시퀀스를 처리하는 데 매우 유용한 네트워크 아키텍쳐인 LSTM을 사용해 DNN의 한계를 보완했습니다. LSTM은 장기간의 시간적 의존성이 있는 데이터에서 성공적으로 학습할 수 있는 능력을 가지고 있으므로, input과 이에 해당하는 output 간의 상당한 시간 지연이 있는 경우에도 유용합니다. 
- LSTM을 사용하여 input sequence를 고정된 차원의 벡터로 인코딩하고, 그 후에 디코딩 단계에서 해당 벡터를 output sequence로 디코딩하여 sequence를 sequence로 매핑합니다. 이를 통해 input sequence와 output sequence의 길이가 가변적인 문제를 해결할 수 있습니다.
![Fig1](/assets/img/fig1.png)
- 그림과 같이 LSTM을 사용해서 input sequence를 <EOS> 토큰이 출력될 때까지 한 스텝씩 학습합니다. <EOS> 토큰이 출력되면 두 번째 LSTM을 사용하여 output sequence를 추출하기 위해 순차적으로 출력합니다. 이 역시 <EOS> 토큰이 출력될 때까지 진행합니다.

### 이 연구의 주요 결과
| - WMT'14 영어-프랑스어 번역 작업에서 384M 매개변수와 8,000 차원 state를 갖는 5개의 deep LSTM의 앙상블에서 번역을 직접 추출하여 left-to-right beam search decoder를 사용하여 BLEU 34.81을 얻었습니다. 이는 대규모 신경망을 사용한 직접 번역으로서 당시 최고의 결과였습니다. 비교하기 위해, 이 데이터 셋에서 구축한 SMT baseline의 BLEU 점수는 33.30입니다.
- 이 점수는 80k 단어로 구성된 어휘를 가진 LSTM으로 달성되었으므로, 아직 최적화되지 않은 작은 어휘 신경망 아키텍처도 구문 기반 SMT 시스템을 능가한다는 것을 보여줍니다.
- 마지막으로, LSTM을 사용하여 동일한 작업에서 SMT baseline의 상위 1000개 번역 리스트를 재점수화했습니다. 이렇게 함으로써, BLEU 점수를 36.5로 개선하였습니다.

### trick
| - 다른 관련 연구와 달리, 아주 긴 문장에 대해서도 좋은 성능을 달성했습니다.
- source sentence의 단어 순서를 거꾸로 뒤집는 방법을 통해 많은 단기 종속성을 도입해 최적화 문제를 단순하게 만들어 긴 문장에 대해서도 학습이 가능했습니다.


- LSTM의 유용한 특성 중 하나는 가변적인 길이의 입력 문장을 고정 차원의 벡터 표현으로 매핑한다는 것입니다. 이를 통해 비슷한 의미를 가진 문장들은 서로 가까이 위치하고, 다른 의미를 가진 문장들은 떨어져 위치합니다. 이 연구가 단어 순서를 인식하고 능동태와 수동태에 대해서도 잘 동작한다는 것을 보여줍니다.

## 2. The model
- RNN은 입력 시퀀스 ( $x_1, ..., x_T$ )가 주어졌을 때, 다음 식을 반복함으로써 출력 시퀀스( $y_1, ..., y_T$ )를 계산합니다.

$$ h_t = sigm{(W^{hx}x_t + W^{hh}h_{t-1}}) $$

$$ y_t = W^{yh}h_t $$

- RNN은 input과 output 간의 정렬이 미리 알려져 있는 경우에는 쉽게 sequence를 sequence로 매핑할 수 있습니다. 그러나 input sequence와 output sequence의 길이가 서로 다르고 복잡한 관계의 문제에서는 장기 의존성을 학습하지 못하는 RNN은 성능이 떨어집니다. 반면에 LSTM은 장기 의존성을 학습할 수 있으므로 이러한 문제에서 효과적입니다.
- LSTM의 목표는 input sequence와 그에 해당하는 output sequence의 조건부 확률 $p({y_1, y_2, \cdots, y_T^` | x_1, x_2, \cdots, x_T})$ 을 추정하는 것입니다.
- LSTM은 먼저 LSTM의 마지막 hidden state에 의해 제공되는 input sequence의 고정 차원 표현 v를 얻은 다음, standard LSTM-LM 공식을 사용하여 $y_1, \cdots, y_T^`$ 의 확률을 계산하여 조건부 확률 p를 추정합니다.

![lstm](/assets/img/lstm.png)
- $y_t$ 는 output sequence의 t번째 단어, v는 input sequence의 고정 차원 표현.
- 각 단어 $y_t$가 주어진 이전단어 $y_1, y_2, ..., y_{i-1}$ 및 input sequence( $x_1, x_2, .., x_T$ )의 정보를 기반으로 확률을 계산합니다.
- 이러한 방식으로 LSTM은 input sequence와 output sequence 사이의 복잡한 관계를 모델링하며, 다양한 길이의 입력 및 출력 sequence를 처리할 수 있습니다.
- 이 식에서, 각 y 분포는 vocabulary에 속한 모든 단어들에 대한 softmax를 사용하여 표현됩니다. 모델이 가능한 모든 길이의 sequence에 대한 분포를 정의할 수 있도록, 각 문장이 특별한 종료 심볼 <EOS>로 끝나야 합니다.

### 실제 모델의 3가지 주요 포인트
1. input sequence와 output sequence에 대해 각각 다른 LSTM을 사용합니다. 이를 통해 모델의 매개변수 수가 거의 증가하지 않으면서 LSTM을 여러 언어 쌍에 동시에 학습하기 쉬워집니다. 서로 다른 파라미터를 가집니다.
2. 깊은 LSTM이 얕은 LSTM보다 훨씬 우수한 성능을 보입니다. 따라서 이 연구에서 4개의 레이어를 가진 LSTM을 사용합니다.
3. 입력 문장의 단어 순서를 뒤집음으로써 성능을 향상시켰습니다. 입력 문장의 순서를 바꾸는 것만으로 모델을 더 쉽게 학습시킬 수 있다는 점에서 학습 효율이 높아 실제 정확도까지 더 높아질 수 있다는 점을 본 논문에서는 경험적으로 입증했습니다.   
