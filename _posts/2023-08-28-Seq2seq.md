---
title: Sequence to Sequence Learning with Neural Networks
date: 2023-08-28 14:49:00 +09:00
categories: [AI, 논문]
tags:
  [ NLP ]
math: true
---
## Abstract

- Deep Neural Networks는 어려운 task에 훌륭한 성능을 달성하는 효과적인 모델입니다.
- DNN(Deep Neural Networks)은 많은 양의 레이블된 학습 데이터 세트가 있는 경우에 효과적이지만, sequence를 sequence로 매핑에는 한계가 있습니다.
  - 즉 입력 sequence와 출력 sequence가 모두 시간적인 관계를 갖는 경우에 사용할 수 없습니다.
    
- 이 논문에서는 sequence 구조를 학습하는 일반적인 end-to-end 방법론을 제시합니다.
- Mylti-layered LSTM을 사용하여 input sequence를 고정된 차원의 벡터로 매핑하고, 해당 벡터에서 target sequence로 디코딩하기 위해 또 다른 LSTM을 사용합니다.
  - LSTM은 장기 의존성을 처리할 수 있기 때문에 sequence에서 패턴을 인식하고 예측할 수 있습니다.
  - LSTM은 단어의 순서를 고려하여 문장의 의미를 파악하며, 능동태와 수동태의 차이와 같은 문법적인 변화에 대해서도 인식할 수 있습니다.
    
- 이 논문에서는 WMT'14 데이터셋을 사용한 영어-프랑스어 번역 작업에서 34.8의 BLEU 점수를 달성합니다.
  - BLEU 점수: machine translation의 성능을 측정하는 데 사용되는 지표 중 하나이며, 값이 높을수록 번역의 질이 더 높다는 것을 의미합니다.

## 1. Introduction

- Deep Neural Networks는 speech recognition, visual object recognition 등 어려운 task에 대해서 훌륭한 성능을 내는 강력한 머신러닝 모델입니다.
- DNN은 임의의 병렬 계산을 적은 수의 단계로 수행할 수 있기 때문에 강력합니다. 그러나 DNN은 input 데이터가 가변적인 길이를 가지는 경우에는 한계점이 존재하기 때문에 input과 output이 고정된 차원의 벡터로 인코딩하는 문제에 대해서만 적용이 가능합니다.
- 예를 들어, speech recognition과 machine translation은 sequential한 문제입니다. 마찬가지로, question answering 역시 질문 sequence를 답변 sequence로 매핑하는 문제입니다. 이러한 sequential한 특성을 고려할 때 DNN의 한계가 드러납니다.
- 이 논문에서는 시퀀스를 처리하는 데 매우 유용한 네트워크 아키텍쳐인 LSTM을 사용해 DNN의 한계를 보완했습니다. LSTM은 장기간의 시간적 의존성이 있는 데이터에서 성공적으로 학습할 수 있는 능력을 가지고 있으므로, input과 이에 해당하는 output 간의 상당한 시간 지연이 있는 경우에도 유용합니다. 
- LSTM을 사용하여 input sequence를 고정된 차원의 벡터로 인코딩하고, 그 후에 디코딩 단계에서 해당 벡터를 output sequence로 디코딩하여 sequence를 sequence로 매핑합니다. 이를 통해 input sequence와 output sequence의 길이가 가변적인 문제를 해결할 수 있습니다.
![Fig1](/assets/img/fig1.png)
- 그림과 같이 LSTM을 사용해서 input sequence를 <EOS> 토큰이 출력될 때까지 한 스텝씩 학습합니다. <EOS> 토큰이 출력되면 두 번째 LSTM을 사용하여 output sequence를 추출하기 위해 순차적으로 출력합니다. 이 역시 <EOS> 토큰이 출력될 때까지 진행합니다.

### 이 연구의 주요 결과
- WMT'14 영어-프랑스어 번역 작업에서 384M 매개변수와 8,000 차원 state를 갖는 5개의 deep LSTM의 앙상블에서 번역을 직접 추출하여 left-to-right beam search decoder를 사용하여 BLEU 34.81을 얻었습니다. 이는 대규모 신경망을 사용한 직접 번역으로서 당시 최고의 결과였습니다. 비교하기 위해, 이 데이터 셋에서 구축한 SMT baseline의 BLEU 점수는 33.30입니다.
- 이 점수는 80k 단어로 구성된 어휘를 가진 LSTM으로 달성되었으므로, 아직 최적화되지 않은 작은 어휘 신경망 아키텍처도 구문 기반 SMT 시스템을 능가한다는 것을 보여줍니다.
- 마지막으로, LSTM을 사용하여 동일한 작업에서 SMT baseline의 상위 1000개 번역 리스트를 재점수화했습니다. 이렇게 함으로써, BLEU 점수를 36.5로 개선하였습니다.

### trick
- 다른 관련 연구와 달리, 아주 긴 문장에 대해서도 좋은 성능을 달성했습니다.
- source sentence의 단어 순서를 거꾸로 뒤집는 방법을 통해 많은 단기 종속성을 도입해 최적화 문제를 단순하게 만들어 긴 문장에 대해서도 학습이 가능했습니다.


- LSTM의 유용한 특성 중 하나는 가변적인 길이의 입력 문장을 고정 차원의 벡터 표현으로 매핑한다는 것입니다. 이를 통해 비슷한 의미를 가진 문장들은 서로 가까이 위치하고, 다른 의미를 가진 문장들은 떨어져 위치합니다. 이 연구가 단어 순서를 인식하고 능동태와 수동태에 대해서도 잘 동작한다는 것을 보여줍니다.

## 2. The model
- RNN은 입력 시퀀스 ( $x_1, ..., x_T$ )가 주어졌을 때, 다음 식을 반복함으로써 출력 시퀀스( $y_1, ..., y_T$ )를 계산합니다.

$$ h_t = sigm{(W^{hx}x_t + W^{hh}h_{t-1}}) $$  

$$ y_t = W^{yh}h_t $$


- RNN은 input과 output 간의 정렬이 미리 알려져 있는 경우에는 쉽게 sequence를 sequence로 매핑할 수 있습니다. 그러나 input sequence와 output sequence의 길이가 서로 다르고 복잡한 관계의 문제에서는 장기 의존성을 학습하지 못하는 RNN은 성능이 떨어집니다. 반면에 LSTM은 장기 의존성을 학습할 수 있으므로 이러한 문제에서 효과적입니다.
- LSTM의 목표는 input sequence와 그에 해당하는 output sequence의 조건부 확률을 추정하는 것입니다.
- $p({y_1, y_2, \cdots, y_T^` | x_1, x_2, \cdots, x_T})$
- LSTM은 먼저 LSTM의 마지막 hidden state에 의해 제공되는 input sequence의 고정 차원 표현 v를 얻은 다음, standard LSTM-LM 공식을 사용하여 $y_1, \cdots, y_T^`$ 의 확률을 계산하여 조건부 확률 p를 추정합니다.

![lstm](/assets/img/lstm.png)
- $y_t$ 는 output sequence의 t번째 단어, v는 input sequence의 고정 차원 표현.
- 각 단어 $y_t$가 주어진 이전단어 $y_1, y_2, ..., y_{i-1}$ 및 input sequence( $x_1, x_2, .., x_T$ )의 정보를 기반으로 확률을 계산합니다.
- 이러한 방식으로 LSTM은 input sequence와 output sequence 사이의 복잡한 관계를 모델링하며, 다양한 길이의 입력 및 출력 sequence를 처리할 수 있습니다.
- 이 식에서, 각 y 분포는 vocabulary에 속한 모든 단어들에 대한 softmax를 사용하여 표현됩니다. 모델이 가능한 모든 길이의 sequence에 대한 분포를 정의할 수 있도록, 각 문장이 특별한 종료 심볼 <EOS>로 끝나야 합니다.

### 실제 모델의 3가지 주요 포인트
1. input sequence와 output sequence에 대해 각각 다른 LSTM을 사용합니다. 이를 통해 모델의 매개변수 수가 거의 증가하지 않으면서 LSTM을 여러 언어 쌍에 동시에 학습하기 쉬워집니다. 서로 다른 파라미터를 가집니다.
2. 깊은 LSTM이 얕은 LSTM보다 훨씬 우수한 성능을 보입니다. 따라서 이 연구에서 4개의 레이어를 가진 LSTM을 사용합니다.
3. 입력 문장의 단어 순서를 뒤집음으로써 성능을 향상시켰습니다. 입력 문장의 순서를 바꾸는 것만으로 모델을 더 쉽게 학습시킬 수 있다는 점에서 학습 효율이 높아 실제 정확도까지 더 높아질 수 있다는 점을 본 논문에서는 경험적으로 입증했습니다.

## 3. Experiments

- 본 연구는 WMT'14 영어-프랑스어 번역 작업에 두 가지 방식으로 적용했습니다. 첫째로, baseline SMT 시스템을 사용하지 않고 입력 문장을 직접 번역했습니다. 둘째로, SMT baseline의 n개의 best 번역 목록을 재점수화했습니다.

### 3.1. Dataset details

- WMT'14 English to French dataset을 사용했습니다.
- 348M 프랑스어 단어와 304M 영어 단어로 구성된 12M개의 문장 부분 집합에 대해 모델 학습했습니다.
- 전형적인 신경망 언어 모델은 각 단어에 대한 벡터 표현을 기반으로 하는데, 이를 위해 두 언어에 대해 고정된 어휘를 사용했습니다. 이렇게 함으로써 각 단어가 벡터로 표현되어 모델이 입력됩니다.
- source 언어에는 가장 빈도가 높은 160,000개의 단어를 사용하였고, target 언어에는 가장 빈도가 높은 80,000개의 단어를 사용했습니다.
- 어휘에 없는 단어는 "UNK" 토큰으로 대체했습니다.  

### 3.2. Decoding and Rescoring

- 이 실험의 핵심은 많은 문장 쌍에서 대규모의 deep LSTM을 학습시키는 것입니다.
- source sequence S가 주어졌을 때 정확한 번역 T의 로그 확률을 최대화하도록 모델을 학습시켰습니다. 
- 따라서 S가 훈련 세트인 경우 훈련 목표는 다음 식과 같습니다.
![train_objective](/assets/img/train_objective.png)

- 학습이 완료되면 LSTM에 따라 가장 가능성이 높은 번역을 찾아서 생성합니다.
![translation](/assets/img/translation.png)
- 간단한 left-to-right beam search decoder를 사용하여 가능한 번역 후보들을 탐색하면서 가장 가능성 있는 번역을 찾습니다.
  - beam search decoder는 번역을 만들 때 사용되는 도구로, 가능한 번역 후보를 찾는 방법입니다. 이 디코더는 여러 개의 가능한 번역을 동시에 고려하며 가장 적합한 번역을 찾아냅니다. 이 디코더는 각각의 가능한 번역을 "부분 가설"이라는 작은 조각으로 쪼갭니다. 그런 다음, 어휘에서 가능한 단어를 가져와서 각 부분 가설에 추가하여 확장합니다. 각 부분 가설의 확장을 평가해서 어떤 번역이 가장 좋은지 알아냅니다. 이렇게 확장장과 평가를 반복하면서 가능한 다양한 번역 후보를 만들어내고, 그 중 가장 확률이 높은 번역 후보를 찾아냄으로써 최적의 번역을 탐색하는 방법입니다.    


- 각 타임스텝에서 beam search decoder는 현재까지 생성된 부분 가설들을 각각 여러 방향으로 확장합니다. 이렇게 하면 가능한 다양한 번역 후보들을 생성하면서 가장 가능성 있는 번역을 찾아가게 됩니다. 

- 이로 인해 가설의 수가 크게 증가하므로 모델의 로그 확률에 따라 B개의 가장 가능성 있는 가설만 유지하고 나머지는 모두 버립니다. <EOS> 기호가 가설에 추가되면 해당 가설은 beam에서 제거되고 완전한 가설의 집합에 추가됩니다.
- 흥미로운 점은 beam 크기가 1일 때에도 시스템이 잘 작동하며, beam 크기가 2인 경우 beam search의 대부분의 장점을 활용할 수 있습니다.
![table1](/assets/img/table1.png)

- Baseline 시스템이 생성한 1000개의 최상의 번역 목록을 재평가하기 위해 각 가설의 로그 확률을 LSTM을 사용하여 계산하고, 기존 점수와 LSTM의 점수를 평균내서 재평가했습니다.

### 3.3. Reversing the Source Sentences

- 이 연구에서는 source sentence를 뒤집었을 때, LSTM 학습이 더 잘 된다는 것을 발견했습니다.
- LSTM의 test perplexity가 5.8에서 4.7로 개선되었고, test BLEU score 역시 25.9에서 30.6으로 개선되었습니다.
- 논문에서는 이 현상에 대해서 정확한 설명을 할 수는 없지만, dataset에 많은 단기 종속성을 도입함으로써 이 현상이 나타난 것 같다고 설명합니다.
- 일반적으로 source 문장의 각 단어는 target 문장의 해당 단어와 멀리 떨어져있기 때문에 큰 "최소 시간 지연"의 문제가 존재합니다.
- source 문장의 단어를 반대로 배치함으로써 source 언어와 target 언어 간의 해당 단어의 평균 거리는 변경되지 않지만, source 언어의 처음 몇개의 단어는 target 언어의 처음 몇 개의 단어와 매우 가까워졌기 때문에 "최소 시간 지연" 문제가 크게 줄어들면서 성능이 되는 것이 아닐까라고 필자는 생각합니다.(앞 부분의 성능이 좋으면 이어지는 부분도 성능이 좋아질 것이라고 생각합니다.)

### 3.4. Training details
- 각 레이어에 1,000개의 cell과 1,000차원의 word embedding이 있는 4개의 layer로 구성된 deep LSTM을 사용했습니다.
- input vocabulary: 160,000
- output vocabulary: 80,000
- 추가 레이어마다 perplexity가 거의 10%씩 줄어든 것으로 보아, deep LSTM이 더 큰 hidden state를 가지고 있기 때문에 shallow LSTM보다 더 좋은 성능을 냅니다.
- 각 출력에서 80,000개의 단어에 대한 naive softmax를 사용합니다.
- 384M개의 파라미터를 가지며, 그 중 64M은 recurrent connections에 속합니다.(32M -> "encoder" LSTM, 32M -> "decoder" LSTM)
- LSTM의 파라미터들은 -0.08 ~ 0.08의 유니폼 분포를 따르도록 초기화.
- 고정된 learning rate 0.7을 사용하는 모멘텀 없는 sgd 사용. 5번의 에포크 후, 반 에포크마다 lr을 절반씩 감소. 총 7.5 에포크 동안 모델 학습.
- batch size: 128
- LSTM은 보통 gradient vanishing 문제가 발생하지 않지만 gradient exploding 문제가 발생할 수 있습니다. gradient의 norm이 threshold를 초과할 때 scaling을 하는 강한 제약 조건을 설정하여 exploding 문제 방지합니다. -> 각 training batch마다, gradient를 128로 나눈 후 norm 제곱을 s로 정의하고 ( $s = ||g||_2$), $s > 5$인 경우에 $g = 5 g/s$.
- 같은 미니배치 안에 포함된 문장들은 최대한 비슷한 길이로 이루어지도록 학습 속도를 높입니다. -> 패딩이 최대한 적게 들어가도록 만들면서 2배 속도 향상

### 3.5. Parallelization
- 이전 섹션에서 구성한 deep LSTM의 구현은 너무 느리기 때문에 8-GPU 머신을 사용하여 모델을 병렬화했습니다. LSTM의 4개 layer가 각각 별도의 GPU에 위치하도록 구성하였고, 나머지 4개의 GPU는 softmax를 병렬화하는 데 사용하여 각 GPU는 1000 x 20000 행렬과의 곱셈을 수행합니다. 이 결과, 128의 미니배치 크기로 초당 6300개의 단어를 처리할 수 있는 속도를 달성하였고 학습은 약 10일 소요되었습니다.

### 3.6. Experimental Results

본 논문의 최고 성능은 여러 개의 LSTM 모델을 앙상블하여 사용한 것입니다. 이 앙상블은 랜덤 초기화 및 랜덤 미니배치 순서를 가진 여러 개의 LSTM 모델로 이루어져 있습니다. LSTM 앙상블의 번역은 Best WMT'14 시스템을 능가하지 못하지만, 어휘 이외의 단어를 처리하지 못한다는 한계에도 불구하고 대규모 machine translation 작업에서 phrase-based SMT baseline을 상당한 차이로 성능을 개선했습니다. LSTM이 baseline system의 1000개 최상의 번역목록을 재점수화하는 데 사용된다면, LSTM은 최고의 WMT'14 결과와 0.5 BLEU 점수 내외로 뒤쳐집니다.  
![table1](/assets/img/table1.png)
![table2](/assets/img/table2.png)

### 3.7. Performance on long sentences
![table3](/assets/img/table3.png)
표 3은 LSTM이 아주 긴 문장에서도 잘 수행된다는 것을 보여줍니다.

### 3.8. Model Analysis
![fig2](/assets/img/fig2.png)
- Figure 2: 그림에서 나타난 구절을 처리한 후 얻은 LSTM hidden state의 2차원 PCA 투영.
- 이 모델의 매력적인 특징 중 하나는 단어 시퀀스를 고정 차원의 벡터로 변환할 수 있는 능력입니다.
- 이 그림은 표현이 단어의 순서에는 민감하게 반응하지만, 능동태에서 수동태로 변경하는 것에는 크게 영향을 받지 않는 것을 보여줍니다.

![fig3](/assets/img/fig3.png)
- 왼쪽 그래프는 문장 길이에 따른 LSTM의 성능을 보여줍니다.
- 35단어 미만의 문장에는 성능 저하가 없으며, 가장 긴 문장에서도 미세한 성능 저하만 있습니다.
- 오른쪽 그래프는 단어 빈도가 점점 더 적은 문장에 대한 LSTM의 성능을 보여줍니다.
